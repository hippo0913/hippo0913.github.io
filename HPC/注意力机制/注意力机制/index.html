<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.0.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":true,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="注意力机制  1. 什么是注意力机制(Attention) Attention本质就是从关注全部到关注重点 1.1. AI 领域的 Attention 机制   Attention机制最早是在计算机视觉里应用的，随后在 NLP(Natural Language Processing)领域也开始应用了，因为2018年BERT(Bidirectional Encoder Representation">
<meta property="og:type" content="article">
<meta property="og:title" content="注意力机制">
<meta property="og:url" content="http://example.com/HPC/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/index.html">
<meta property="og:site_name" content="铁臂IT狗">
<meta property="og:description" content="注意力机制  1. 什么是注意力机制(Attention) Attention本质就是从关注全部到关注重点 1.1. AI 领域的 Attention 机制   Attention机制最早是在计算机视觉里应用的，随后在 NLP(Natural Language Processing)领域也开始应用了，因为2018年BERT(Bidirectional Encoder Representation">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/HPC/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/1ab35-2019-11-13-attention-encoderdecoder.gif">
<meta property="article:published_time" content="2023-12-18T04:04:22.000Z">
<meta property="article:modified_time" content="2023-12-20T10:05:55.097Z">
<meta property="article:author" content="铁臂IT狗">
<meta property="article:tag" content="HPC">
<meta property="article:tag" content="CONCEPTION">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/HPC/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/1ab35-2019-11-13-attention-encoderdecoder.gif">


<link rel="canonical" href="http://example.com/HPC/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/HPC/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","path":"HPC/注意力机制/注意力机制/","title":"注意力机制"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>注意力机制 | 铁臂IT狗</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">铁臂IT狗</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">铁臂IT狗 技术博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container">
  <div class="algolia-stats"><hr></div>
  <div class="algolia-hits"></div>
  <div class="algolia-pagination"></div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">1.</span> <span class="nav-text">注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6attention"><span class="nav-number">1.1.</span> <span class="nav-text">1. 什么是注意力机制(Attention)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ai-%E9%A2%86%E5%9F%9F%E7%9A%84-attention-%E6%9C%BA%E5%88%B6"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1. AI 领域的 Attention 机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention%E7%9A%84%E4%B8%89%E5%A4%A7%E4%BC%98%E7%82%B9"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2. Attention的三大优点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention%E7%9A%84%E5%8E%9F%E7%90%86"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 Attention的原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention%E5%8E%9F%E7%90%86%E4%B8%89%E6%AD%A5%E5%88%86%E8%A7%A3"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4 Attention原理三步分解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention-%E7%9A%84-n-%E7%A7%8D%E7%B1%BB%E5%9E%8B"><span class="nav-number">1.1.5.</span> <span class="nav-text">1.5. Attention 的 N 种类型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%8C%BA%E5%9F%9F"><span class="nav-number">1.1.5.1.</span> <span class="nav-text">1.5.1. 计算区域</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%80%E7%94%A8%E4%BF%A1%E6%81%AF"><span class="nav-number">1.1.5.2.</span> <span class="nav-text">1.5.2. 所用信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%80%E7%94%A8%E4%BF%A1%E6%81%AF-1"><span class="nav-number">1.1.5.3.</span> <span class="nav-text">1.5.3. 所用信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%96%B9%E9%9D%A2"><span class="nav-number">1.1.5.4.</span> <span class="nav-text">1.5.4. 模型方面</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">1.1.6.</span> <span class="nav-text">1.1. 什么是注意力机制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%8F%8A%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-number">1.2.</span> <span class="nav-text">2. 自注意力机制及优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1. 自注意力机制的优点：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2. 自注意力机制的缺点：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%83%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E5%8F%8A%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-number">1.3.</span> <span class="nav-text">3. 它注意力机制及优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%83%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1. 它注意力机制的优点：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%83%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2. 它注意力机制的缺点：</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">铁臂IT狗</p>
  <div class="site-description" itemprop="description">hippo 技术归档</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/HPC/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="铁臂IT狗">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="铁臂IT狗">
      <meta itemprop="description" content="hippo 技术归档">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="注意力机制 | 铁臂IT狗">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          注意力机制
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-12-18 12:04:22" itemprop="dateCreated datePublished" datetime="2023-12-18T12:04:22+08:00">2023-12-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-12-20 18:05:55" itemprop="dateModified" datetime="2023-12-20T18:05:55+08:00">2023-12-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/HPC/" itemprop="url" rel="index"><span itemprop="name">HPC</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/HPC/CONCEPTION/" itemprop="url" rel="index"><span itemprop="name">CONCEPTION</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="注意力机制"><strong>注意力机制</strong></h1>
<p><embed src="5e48e-2019-11-06-banner.png.webp" /></p>
<h2 id="什么是注意力机制attention">1. 什么是注意力机制(Attention)</h2>
<p><code>Attention</code>本质就是<strong>从关注全部到关注重点</strong></p>
<h3 id="ai-领域的-attention-机制">1.1. AI 领域的 <code>Attention</code> 机制</h3>
<p>  <code>Attention</code>机制最早是在计算机视觉里应用的，随后在 <code>NLP</code>(Natural Language Processing)领域也开始应用了，因为2018年<code>BERT</code>(Bidirectional Encoder Representation from Transformers)和<code>GPT</code>(Generative Pre-Trained Transformer)的效果出奇的好。</p>
<p>  如果用图来表达<code>Attention</code>的位置大致是<a href="#picture-1.1">下面</a>的样子：</p>
<p><span id="picture-1.1"></span></p>
<figure>
<embed src="d3164-2019-11-07-weizhi.png.webp" /><figcaption>Attention宏观概念</figcaption>
</figure>
<h3 id="attention的三大优点">1.2. <code>Attention</code>的三大优点</h3>
<ol type="1">
<li>参数少：模型复杂度跟 <code>CNN</code>(Convolutional Neural Network 卷积神经网络)、<code>RNN</code>(Recurrent Neural Network 循环神经网络) 相比，复杂度更小，参数也更少。所以对算力的要求也就更小。</li>
<li>速度快：<code>Attention</code>解决了<code>RNN</code>不能并行计算的问题。<code>Attention</code>机制每一步计算不依赖于上一步的计算结果，因此可以和<code>CNN</code>一样并行处理。</li>
<li>效果好：在<code>Attention</code>机制引入之前，有一个问题大家一直很苦恼：长距离的信息会被弱化，就好像记忆能力弱的人，记不住过去的事情是一样的。<code>Attention</code>是挑重点，就算文本比较长，也能从中间抓住重点，不丢失重要的信息。</li>
</ol>
<h3 id="attention的原理">1.3 <code>Attention</code>的原理</h3>
<p>  <code>Attention</code>经常会和<code>Encoder–Decoder</code>一起说。<a href="#picture-1.2">下面的动图</a>演示了<code>Attention</code>引入<code>Encoder-Decoder</code>框架下，完成机器翻译任务的大致流程。</p>
<p><span id="picture-1.2"></span></p>
<figure>
<img src="1ab35-2019-11-13-attention-encoderdecoder.gif" alt="Attention引入Encoder-Decoder 流程图" /><figcaption>Attention引入Encoder-Decoder 流程图</figcaption>
</figure>
<p><strong>但是，Attention 并不一定要在 Encoder-Decoder 框架下使用的，他是可以脱离 Encoder-Decoder 框架的。</strong></p>
<p>  <a href="#picture-1.3">下面的图片</a>则是脱离 Encoder-Decoder 框架后的原理图解。</p>
<p><span id="picture-1.3"></span></p>
<p><embed src="48015-2019-11-13-only-attention.png.webp" /></p>
<h3 id="attention原理三步分解">1.4 <code>Attention</code>原理三步分解</h3>
<p><span id="picture-1.4"></span></p>
<p><embed src="efa5b-2019-11-13-3step.png.webp" /></p>
<p>第一步： query 和 key 进行相似度计算，得到权值 第二步：将权值进行归一化，得到直接可用的权重 第三步：将权重和 value 进行加权求和</p>
<p>  从上面的建模，我们可以大致感受到 <code>Attention</code> 的思路简单，<strong>四个字“带权求和”就可以高度概括</strong>，大道至简。做个不太恰当的类比，人类学习一门新语言基本经历四个阶段：死记硬背（通过阅读背诵学习语法练习语感）-&gt;提纲挈领（简单对话靠听懂句子中的关键词汇准确理解核心意思）-&gt;融会贯通（复杂对话懂得上下文指代、语言背后的联系，具备了举一反三的学习能力）-&gt;登峰造极（沉浸地大量练习）。 这也如同<code>Attention</code>的发展脉络，<code>RNN</code>时代是死记硬背的时期，<code>Attention</code>的模型学会了提纲挈领，进化到 transformer，融汇贯通，具备优秀的表达学习能力，再到 GPT、BERT，通过多任务大规模学习积累实战经验，战斗力爆棚。 要回答为什么 attention 这么优秀？是因为它让模型开窍了，懂得了提纲挈领，学会了融会贯通。 ——阿里技术</p>
<p>想要了解更多技术细节，可以看看下面的文章或者视频：</p>
<p>“文章”<a target="_blank" rel="noopener" href="https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216">深度学习中的注意力机制</a></p>
<p>“文章”<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/77307258">遍地开花的 Attention，你真的懂吗？</a></p>
<p>“文章”<a target="_blank" rel="noopener" href="https://www.infoq.cn/article/lteUOi30R4uEyy740Ht2">探索 NLP 中的 Attention 注意力机制及 Transformer 详解</a></p>
<p>“视频”<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av56239558?from=search&amp;seid=14406218127146760248">李宏毅 – transformer</a></p>
<p>“视频”<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av56235038?from=search&amp;seid=9558641265797595207">李宏毅 – ELMO、BERT、GPT 讲解</a></p>
<h3 id="attention-的-n-种类型">1.5. <code>Attention</code> 的 N 种类型</h3>
<p>  <code>Attention</code> 有很多种不同的类型：Soft Attention、Hard Attention、静态Attention、动态Attention、Self Attention 等等。下面就跟大家解释一下这些不同的 Attention 都有哪些差别。</p>
<p><span id="picture-1.5"></span></p>
<figure>
<embed src="7fc4f-2019-11-13-types.png.webp" /><figcaption>Attention的N种类型</figcaption>
</figure>
<p>  本节从计算区域、所用信息、结构层次和模型等方面对<code>Attention</code>的形式进行归类。</p>
<h4 id="计算区域">1.5.1. 计算区域</h4>
<p>  根据Attention的计算区域，可以分成以下几种：</p>
<ol type="1">
<li>Soft Attention，这是比较常见的Attention方式，对所有key求权重概率，每个key都有一个对应的权重，是一种全局的计算方式（也可以叫Global Attention）。这种方式比较理性，参考了所有key的内容，再进行加权。但是计算量可能会比较大一些。</li>
<li>Hard Attention，这种方式是直接精准定位到某个key，其余key就都不管了，相当于这个key的概率是1，其余key的概率全部是0。因此这种对齐方式要求很高，要求一步到位，如果没有正确对齐，会带来很大的影响。另一方面，因为不可导，一般需要用强化学习的方法进行训练。（或者使用gumbel softmax之类的）</li>
<li>Local Attention，这种方式其实是以上两种方式的一个折中，对一个窗口区域进行计算。先用Hard方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用Soft方式来算Attention。</li>
</ol>
<h4 id="所用信息">1.5.2. 所用信息</h4>
<p>  假设我们要对一段原文计算Attention，这里原文指的是我们要做attention的文本，那么所用信息包括内部信息和外部信息，内部信息指的是原文本身的信息，而外部信息指的是除原文以外的额外信息。</p>
<blockquote>
<ol type="1">
<li><code>General Attention</code>，这种方式利用到了外部信息，常用于需要构建两段文本关系的任务，query一般包含了额外信息，根据外部query对原文进行对齐。 比如在阅读理解任务中，需要构建问题和文章的关联，假设现在baseline是，对问题计算出一个问题向量q，把这个q和所有的文章词向量拼接起来，输入到 <code>LSTM</code> (Long Short-Term Memory, <strong>长短时记忆网络</strong>)中进行建模。那么在这个模型中，文章所有词向量共享同一个问题向量，现在我们想让文章每一步的词向量都有一个不同的问题向量，也就是，在每一步使用文章在该步下的词向量对问题来算attention，这里问题属于原文，文章词向量就属于外部信息。</li>
<li><code>Local Attention</code>，这种方式只使用内部信息，key和value以及query只和输入原文有关，在self attention中，key=value=query。既然没有外部信息，那么在原文中的每个词可以跟该句子中的所有词进行Attention计算，相当于寻找原文内部的关系。 还是举阅读理解任务的例子，上面的baseline中提到，对问题计算出一个向量q，那么这里也可以用上attention，只用问题自身的信息去做attention，而不引入文章信息。</li>
</ol>
</blockquote>
<h4 id="所用信息-1">1.5.3. 所用信息</h4>
<p>  结构方面根据是否划分层次关系，分为单层attention，多层attention和多头attention：</p>
<blockquote>
<ol type="1">
<li>单层Attention，这是比较普遍的做法，用一个query对一段原文进行一次attention。</li>
<li>多层Attention，一般用于文本具有层次关系的模型，假设我们把一个document划分成多个句子，在第一层，我们分别对每个句子使用attention计算出一个句向量（也就是单层attention）；在第二层，我们对所有句向量再做attention计算出一个文档向量（也是一个单层attention），最后再用这个文档向量去做任务。</li>
<li>多头Attention，这是《Attention is All You Need》中提到的multi-head attention，用到了多个query对一段原文进行了多次attention，每个query都关注到原文的不同部分，相当于重复做多次单层attention：</li>
</ol>
</blockquote>
<p><span class="math inline">\(head_i = Attention(q_i, K, V)\)</span></p>
<p>最后再把这些结果拼接起来：</p>
<p><span class="math inline">\(MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\)</span></p>
<h4 id="模型方面">1.5.4. 模型方面</h4>
<p>从模型上看，Attention一般用在CNN和LSTM上，也可以直接进行纯Attention计算。</p>
<h3 id="什么是注意力机制">1.1. 什么是注意力机制</h3>
<p>  注意力机制（Attention Mechanism）是一种用于处理和解决信息超载问题的资源分配方案，它可以将计算资源分配给更重要的任务。这种机制可以应用于任何类型的输入，而不管其形状如何。在机器学习中，注意力机制已经广泛应用于自然语言处理、图像识别及语音识别等各种不同类型的机器学习任务中。</p>
<p>  注意力机制可以模拟人类在处理信息时的选择性关注和记忆机制，它通过计算输入序列中不同位置之间的相关性得分，得到一个权重分布，然后使用这个权重分布对输入序列进行加权求和，得到一个加权后的表示。这个表示可以看作是输入序列的压缩表示，它包含了输入序列的重要信息和上下文关系。</p>
<p>  注意力机制可以分为自注意力机制和它注意力机制两种类型。自注意力机制通过计算输入序列中不同位置之间的相关性得分来得到权重分布，它适用于需要捕捉输入序列内部相关性的任务。它注意力机制则通过计算输入序列与查询之间的相似度来得到权重分布，它适用于需要比较输入序列与查询之间相似度的任务。</p>
<p>  总之，注意力机制是一种有效的数据处理方法，它可以提高模型的效率和准确性，是自然语言处理领域的一个重要研究方向。</p>
<h2 id="自注意力机制及优缺点">2. 自注意力机制及优缺点</h2>
<p>  自注意力机制是一种特殊的注意力机制，它允许模型在处理一个序列时，考虑到序列中每个元素与其他所有元素的关系。这种机制可以帮助模型更好地理解序列中的上下文信息，从而更准确地处理序列数据。</p>
<h3 id="自注意力机制的优点">2.1. 自注意力机制的优点：</h3>
<ol type="1">
<li>可以建立全局的依赖关系，扩大图像的感受野。相比于CNN，其感受野更大，可以获取更多上下文信息。</li>
<li>可以通过大量数据进行学习，建立准确的全局关系。</li>
<li>参数少，对算力的要求也就更小。</li>
<li>可以并行计算，解决RNN及其变体模型不能并行计算的问题。</li>
<li>捕捉长程依赖：自注意力机制可以捕捉到输入序列中的长程依赖关系，这是传统的循环神经网络难以做到的。</li>
<li>并行计算：自注意力机制的计算过程是并行的，可以大大提高计算效率。</li>
<li>捕捉全局信息：自注意力机制通过计算输入序列中不同位置之间的相关性得分，得到一个权重分布，可以捕捉到输入序列的全局信息。</li>
</ol>
<h3 id="自注意力机制的缺点">2.2. 自注意力机制的缺点：</h3>
<ol type="1">
<li>自注意力机制是通过筛选重要信息，过滤不重要信息实现的，这就导致其有效信息的抓取能力会比CNN小一些。</li>
<li>自注意力机制相比CNN，无法利用图像本身具有的尺度，平移不变性，以及图像的特征局部性这些先验知识。</li>
<li>计算复杂度高：自注意力机制的计算复杂度较高，需要大量的计算资源。</li>
<li>容易过拟合：自注意力机制在训练过程中需要计算输入序列中不同位置之间的相关性得分，如果训练数据不足或者模型参数设置不当，容易导致过拟合。</li>
<li>需要大量数据：自注意力机制需要大量的数据来训练模型，如果数据量不足，模型的性能可能会受到影响。</li>
</ol>
<h2 id="它注意力机制及优缺点">3. 它注意力机制及优缺点</h2>
<p>  它注意力机制通过计算输入序列与查询之间的相似度来得到权重分布，它适用于需要比较输入序列与查询之间相似度的任务。</p>
<h3 id="它注意力机制的优点">3.1. 它注意力机制的优点：</h3>
<ol type="1">
<li>它可以计算输入序列与查询之间的相似度，从而得到权重分布。</li>
<li>它可以帮助模型更好地理解输入序列与查询之间的相似度，从而更准确地处理序列数据。</li>
</ol>
<h3 id="它注意力机制的缺点">3.2. 它注意力机制的缺点：</h3>
<ol type="1">
<li>它需要额外的计算资源来计算输入序列与查询之间的相似度。</li>
<li>如果输入序列与查询之间的相似度计算不准确，它可能会影响模型的性能。</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/HPC/" rel="tag"># HPC</a>
              <a href="/tags/CONCEPTION/" rel="tag"># CONCEPTION</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/TOOLS/ssh%E5%BF%83%E5%BE%97/ssh%E5%BF%83%E5%BE%97/" rel="prev" title="ssh心得">
                  <i class="fa fa-angle-left"></i> ssh心得
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/TOOLS/markdown/markdown/" rel="next" title="markdown">
                  markdown <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2023</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">铁臂IT狗</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/algoliasearch/4.20.0/algoliasearch-lite.umd.js" integrity="sha256-DABVk+hYj0mdUzo+7ViJC6cwLahQIejFvC+my2M/wfM=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/instantsearch.js/4.60.0/instantsearch.production.min.js" integrity="sha256-9242vN47QUX50UG5Gf5XDO1YREWCEJRyXHofh5fsl24=" crossorigin="anonymous"></script><script src="/js/third-party/search/algolia-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"mhchem":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
